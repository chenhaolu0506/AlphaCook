{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e274fdcc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from transformer import PositionalEncoding\n",
    "from model import ImageCaptionModel, accuracy_function, loss_function\n",
    "from decoder import TransformerDecoder, RNNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3ab5cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import   transformer, model, decoder\n",
    "%aimport transformer, model, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "223ac6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './YouCookII/'\n",
    "anno_path = root_path + \"annotations/youcookii_annotations_trainval.json\"\n",
    "feat_path = root_path + 'features/feat_csv/'\n",
    "\n",
    "feat_path_keywords = {'train': 'train_frame_feat_csv', 'val': 'val_frame_feat_csv', 'test': 'test_frame_feat_csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1cf7b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(anno_path, 'rb')\n",
    "anno_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f82592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_types = pd.read_csv(root_path + 'label_foodtype.csv', header=None)\n",
    "idx, types = food_types[0], food_types[1]\n",
    "idx2type = {i: t for i, t in zip(idx, types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a0029fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = feat_path + feat_path_keywords['train']\n",
    "val_dir = feat_path + feat_path_keywords['val']\n",
    "test_dir = feat_path + feat_path_keywords['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7e8a274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': 311.77,\n",
       " 'subset': 'training',\n",
       " 'recipe_type': '101',\n",
       " 'annotations': [{'segment': [41, 54],\n",
       "   'id': 0,\n",
       "   'sentence': 'place the bacon slices on a baking pan and cook them in an oven'},\n",
       "  {'segment': [84, 122],\n",
       "   'id': 1,\n",
       "   'sentence': 'cut the tomatoes into thin slices'},\n",
       "  {'segment': [130, 135],\n",
       "   'id': 2,\n",
       "   'sentence': 'toast the bread slices in the toaster'},\n",
       "  {'segment': [147, 190],\n",
       "   'id': 3,\n",
       "   'sentence': 'spread mayonnaise on the bread and place bacon slices lettuce and tomato slices on top'},\n",
       "  {'segment': [192, 195], 'id': 4, 'sentence': 'top the sandwich with bread'}],\n",
       " 'video_url': 'https://www.youtube.com/watch?v=0O4bxhpFX9o'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_dict['database']['0O4bxhpFX9o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e7f780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dirs):\n",
    "    all_frames = []\n",
    "    all_caps = []\n",
    "    for dir_name in dirs:\n",
    "        all_vid_names = os.listdir(dir_name)\n",
    "        for vid in all_vid_names:\n",
    "            if vid == '.DS_Store':\n",
    "                continue\n",
    "            vid_dir = dir_name + '/' + vid\n",
    "            vid_feat1 = pd.read_csv(dir_name + '/' + vid + '/0001/resnet_34_feat_mscoco.csv', \n",
    "                                   header=None)\n",
    "            feats = [vid_feat1]\n",
    "            for vid_feat in feats:\n",
    "                vid_segs = anno_dict['database'][vid]['annotations']\n",
    "                vid_len = anno_dict['database'][vid]['duration']\n",
    "                samp_rate = vid_len / 500\n",
    "                num_segs = len(vid_segs)\n",
    "                for segments in vid_segs:\n",
    "                    start, end = segments['segment']\n",
    "                    cap = segments['sentence']\n",
    "                    start_fr = int(np.ceil(start / samp_rate))\n",
    "                    end_fr = int(np.floor(end / samp_rate))\n",
    "                    frame_idx = []\n",
    "                    if end_fr - start_fr < 5:\n",
    "                        frame_idx = list(range(start_fr, end_fr+1))\n",
    "                        while len(frame_idx) < 5:\n",
    "                            frame_idx += random.sample(list(range(start_fr, end_fr+1)), 1)\n",
    "                    else:\n",
    "                        frame_idx = random.sample(list(range(start_fr, end_fr+1)), 5)\n",
    "                    for frame_num in frame_idx:\n",
    "                        frame = vid_feat.iloc[frame_num]\n",
    "                        all_frames.append(frame.to_numpy())\n",
    "                        all_caps.append(cap)\n",
    "    return np.array(all_frames), np.array(all_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393f3ce",
   "metadata": {},
   "source": [
    "# Preprocessing Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "293b4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "06eddc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions, window_size):\n",
    "    caps_ret = []\n",
    "    for i, caption in enumerate(captions):\n",
    "        # Taken from:\n",
    "        # https://towardsdatascience.com/image-captions-with-attention-in-tensorflow-step-by-step-927dad3569fa\n",
    "\n",
    "        # Convert the caption to lowercase, and then remove all special characters from it\n",
    "        # caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
    "        # TODO: this step can be handled with keras tokenizer?\n",
    "\n",
    "        # Split the caption into separate words, and collect all words which are more than \n",
    "        # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
    "        clean_words = []\n",
    "        for word in caption.split():\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "            elif word.isnumeric():\n",
    "                clean_words.append('<num>')\n",
    "\n",
    "        # Join those words into a string\n",
    "        caption_new = ['<start>'] + clean_words[:window_size-1] + ['<end>']\n",
    "\n",
    "        # Replace the old caption in the captions list with this new cleaned caption\n",
    "        caps_ret.append(caption_new)\n",
    "    return caps_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8038f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(captions, vocab_size):\n",
    "    word_count = collections.Counter()\n",
    "    for caption in captions:\n",
    "        word_count.update(caption)\n",
    "    \n",
    "    vocab = [word for word, count in word_count.most_common(vocab_size)]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9da0ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_captions(captions, vocab):\n",
    "    temp = copy.deepcopy(captions)\n",
    "    for caption in temp:\n",
    "        for index, word in enumerate(caption):\n",
    "            if word not in vocab:\n",
    "                caption[index] = '<unk>'\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cfa9c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_captions(captions, window_size):\n",
    "    pad_cap = copy.deepcopy(captions)\n",
    "    for caption in pad_cap:\n",
    "        caption += (window_size + 1 - len(caption)) * ['<pad>']\n",
    "    return pad_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "74e0e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_all(captions_train, captions_test, window_size):\n",
    "    clean_caps_train = preprocess_captions(captions_train, window_size)\n",
    "    clean_caps_test = preprocess_captions(captions_test, window_size)\n",
    "    \n",
    "    vocab = build_vocab(clean_caps_train, 1800)\n",
    "    \n",
    "    masked_caps_train = unk_captions(clean_caps_train, vocab)\n",
    "    masked_caps_test = unk_captions(clean_caps_test, vocab)\n",
    "    \n",
    "    padded_caps_train = pad_captions(masked_caps_train, window_size)\n",
    "    padded_caps_test = pad_captions(masked_caps_test, window_size)\n",
    "    \n",
    "    word2idx = {}\n",
    "    vocab_size = 0\n",
    "    for caption in padded_caps_train:\n",
    "        for index, word in enumerate(caption):\n",
    "            if word in word2idx:\n",
    "                caption[index] = word2idx[word]\n",
    "            else:\n",
    "                word2idx[word] = vocab_size\n",
    "                caption[index] = vocab_size\n",
    "                vocab_size += 1\n",
    "    for caption in padded_caps_test:\n",
    "        for index, word in enumerate(caption):\n",
    "            caption[index] = word2idx[word] \n",
    "    \n",
    "    return np.array(padded_caps_train), np.array(padded_caps_test), word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c1fb9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subdir = os.listdir(train_dir)\n",
    "train_subdir.remove('.DS_Store')\n",
    "train_paths = [os.path.join(train_dir, subdir) for subdir in train_subdir]\n",
    "val_paths = [os.path.join(val_dir, subdir) for subdir in train_subdir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2ceaa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frames, train_caps = get_data(train_paths)\n",
    "val_frames, val_caps = get_data(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cc85fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caps_token, val_caps_token, word2idx = preproc_all(train_caps, val_caps, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e1cc323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51685,) (17460,)\n",
      "(51685, 21) (17460, 21)\n"
     ]
    }
   ],
   "source": [
    "print(train_caps.shape, val_caps.shape)\n",
    "print(train_caps_token.shape, val_caps_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "84058bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, word2idx, epochs, batch_size, hidden_size, window_size, valid=None):\n",
    "\n",
    "    decoder = TransformerDecoder(\n",
    "        vocab_size  = len(word2idx), \n",
    "        hidden_size = hidden_size, \n",
    "        window_size = window_size\n",
    "    )\n",
    "\n",
    "    model = ImageCaptionModel(decoder)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
    "    model.compile(\n",
    "        optimizer   = optimizer,\n",
    "        loss        = loss_function,\n",
    "        metrics     = [accuracy_function]\n",
    "    )\n",
    "\n",
    "    stats = []\n",
    "    for epoch in range(epochs):\n",
    "        stats += [model.train(train[0], train[1], word2idx['<pad>'], batch_size=batch_size)]\n",
    "        if valid:\n",
    "            model.test(valid[0], valid[1], word2idx['<pad>'], batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "e6cc4b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training 172/172]\t loss=4.965\t acc: 0.123\t perp: 143.338\n",
      "[Valid 58/58]\t loss=4.711\t acc: 0.124\t perp: 111.151\n",
      "[Training 172/172]\t loss=4.076\t acc: 0.246\t perp: 58.880\n",
      "[Valid 58/58]\t loss=3.864\t acc: 0.270\t perp: 47.661\n",
      "[Training 172/172]\t loss=3.538\t acc: 0.298\t perp: 34.403\n",
      "[Valid 58/58]\t loss=3.535\t acc: 0.305\t perp: 34.282\n",
      "[Training 172/172]\t loss=3.242\t acc: 0.328\t perp: 25.593\n",
      "[Valid 58/58]\t loss=3.412\t acc: 0.323\t perp: 30.316\n",
      "[Training 172/172]\t loss=3.052\t acc: 0.348\t perp: 21.166\n",
      "[Valid 58/58]\t loss=3.339\t acc: 0.332\t perp: 28.184\n"
     ]
    }
   ],
   "source": [
    "model = train_model((train_caps_token, train_frames), \n",
    "                    word2idx, \n",
    "                    epochs=5,\n",
    "                    batch_size=300, \n",
    "                    hidden_size=128, \n",
    "                    window_size=window_size, \n",
    "                    valid=(val_caps_token, val_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "456db448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add the meat to the pan\n",
      "add the meat to the pan\n",
      "add the meat to the pan\n",
      "place the sushi rice on the rice\n",
      "add the mixture to the pan\n",
      "add the meat to the pan\n",
      "add the meat to the pan\n",
      "add the meat to the pan\n",
      "add the cooked rice to the pan\n",
      "place the salmon on the tortilla\n"
     ]
    }
   ],
   "source": [
    "def gen_caption_temperature(model, image_embedding, wordToIds, padID, temp, window_length):\n",
    "    \"\"\"\n",
    "    Function used to generate a caption using an ImageCaptionModel given\n",
    "    an image embedding. \n",
    "    \"\"\"\n",
    "    idsToWords = {id: word for word, id in wordToIds.items()}\n",
    "    unk_token = wordToIds['<unk>']\n",
    "    caption_so_far = [wordToIds['<start>']]\n",
    "    while len(caption_so_far) < window_length and caption_so_far[-1] != wordToIds['<end>']:\n",
    "        caption_input = np.array([caption_so_far + ((window_length - len(caption_so_far)) * [padID])])\n",
    "        logits = model(np.expand_dims(image_embedding, 0), caption_input)\n",
    "        logits = logits[0][len(caption_so_far) - 1]\n",
    "        probs = tf.nn.softmax(logits / temp).numpy()\n",
    "        next_token = unk_token\n",
    "        attempts = 0\n",
    "        while next_token == unk_token and attempts < 5:\n",
    "            next_token = np.random.choice(len(probs), p=probs)\n",
    "            attempts += 1\n",
    "        caption_so_far.append(next_token)\n",
    "    return ' '.join([idsToWords[x] for x in caption_so_far][1:-1])\n",
    "\n",
    "temperature = .05\n",
    "for i in range(10):\n",
    "    print(gen_caption_temperature(model, val_frames[i], word2idx, word2idx['<pad>'], temperature, window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "30c6c76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['place a sushi seaweed on the bamboo mat',\n",
       "       'place a sushi seaweed on the bamboo mat',\n",
       "       'place a sushi seaweed on the bamboo mat',\n",
       "       'spread some sushi rice on top of the seaweed',\n",
       "       'spread some sushi rice on top of the seaweed',\n",
       "       'spread some sushi rice on top of the seaweed',\n",
       "       'put mayonnaise fish roe crab meat red lettuce cucumber salmon and avocado on top',\n",
       "       'put mayonnaise fish roe crab meat red lettuce cucumber salmon and avocado on top',\n",
       "       'put mayonnaise fish roe crab meat red lettuce cucumber salmon and avocado on top',\n",
       "       'hold the ingredients and roll them up with the mat'],\n",
       "      dtype='<U201')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_caps[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
