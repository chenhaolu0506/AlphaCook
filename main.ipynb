{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e274fdcc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import copy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223ac6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './YouCookII/'\n",
    "anno_path = root_path + \"annotations/youcookii_annotations_trainval.json\"\n",
    "feat_path = root_path + 'features/feat_csv/'\n",
    "\n",
    "feat_path_keywords = {'train': 'train_frame_feat_csv', 'val': 'val_frame_feat_csv', 'test': 'test_frame_feat_csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf7b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(anno_path, 'rb')\n",
    "anno_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_types = pd.read_csv(root_path + 'label_foodtype.csv', header=None)\n",
    "idx, types = food_types[0], food_types[1]\n",
    "idx2type = {i: t for i, t in zip(idx, types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0029fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_dir = feat_path + feat_path_keywords['train'] + '/101'\n",
    "feat = pd.read_csv(demo_dir + '/0O4bxhpFX9o/0001/resnet_34_feat_mscoco.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8a274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': 311.77,\n",
       " 'subset': 'training',\n",
       " 'recipe_type': '101',\n",
       " 'annotations': [{'segment': [41, 54],\n",
       "   'id': 0,\n",
       "   'sentence': 'place the bacon slices on a baking pan and cook them in an oven'},\n",
       "  {'segment': [84, 122],\n",
       "   'id': 1,\n",
       "   'sentence': 'cut the tomatoes into thin slices'},\n",
       "  {'segment': [130, 135],\n",
       "   'id': 2,\n",
       "   'sentence': 'toast the bread slices in the toaster'},\n",
       "  {'segment': [147, 190],\n",
       "   'id': 3,\n",
       "   'sentence': 'spread mayonnaise on the bread and place bacon slices lettuce and tomato slices on top'},\n",
       "  {'segment': [192, 195], 'id': 4, 'sentence': 'top the sandwich with bread'}],\n",
       " 'video_url': 'https://www.youtube.com/watch?v=0O4bxhpFX9o'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_dict['database']['0O4bxhpFX9o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f519848c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_vid_names = os.listdir(demo_dir)\n",
    "vid2cap = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16fff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = []\n",
    "all_caps = []\n",
    "for vid in all_vid_names:\n",
    "    vid2cap[vid] = {'frames': [], 'cap': []}\n",
    "    if vid == '.DS_Store':\n",
    "        continue\n",
    "    vid_feat = pd.read_csv(demo_dir + '/' + vid + '/0001/resnet_34_feat_mscoco.csv', header=None)\n",
    "    vid_segs = anno_dict['database'][vid]['annotations']\n",
    "    vid_len = anno_dict['database'][vid]['duration']\n",
    "    samp_rate = vid_len / 500\n",
    "    for segments in vid_segs:\n",
    "        start, end = segments['segment']\n",
    "        cap = segments['sentence']\n",
    "        start_fr = int(np.ceil(start / samp_rate))\n",
    "        end_fr = int(np.floor(end / samp_rate))\n",
    "        for frame_num in range(start_fr, end_fr + 1):\n",
    "            frame = vid_feat.iloc[frame_num]\n",
    "            all_frames.append(frame.to_numpy())\n",
    "            all_caps.append(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393f3ce",
   "metadata": {},
   "source": [
    "# Preprocessing Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06eddc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions):\n",
    "    caps_ret = []\n",
    "    for i, caption in enumerate(captions):\n",
    "        # Taken from:\n",
    "        # https://towardsdatascience.com/image-captions-with-attention-in-tensorflow-step-by-step-927dad3569fa\n",
    "\n",
    "        # Convert the caption to lowercase, and then remove all special characters from it\n",
    "        # caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
    "        # TODO: this step can be handled with keras tokenizer?\n",
    "\n",
    "        # Split the caption into separate words, and collect all words which are more than \n",
    "        # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
    "        clean_words = []\n",
    "        for word in caption.split():\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "            elif word.isnumeric():\n",
    "                clean_words.append('<num>')\n",
    "        #         clean_words = [word for word in caption_nopunct.split() if ((len(word) > 1) and (word.isalpha()))]\n",
    "\n",
    "        # Join those words into a string\n",
    "        caption_new = ['<start>'] + clean_words + ['<end>']\n",
    "\n",
    "        # Replace the old caption in the captions list with this new cleaned caption\n",
    "        caps_ret.append(caption_new)\n",
    "    return caps_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1078a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_caps = preprocess_captions(all_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e9bb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03badee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <start>: 3650\n",
      "2. <end>: 3650\n",
      "3. the: 3140\n",
      "4. and: 2438\n",
      "5. on: 2204\n",
      "6. bread: 1405\n",
      "7. bacon: 1381\n",
      "8. of: 1290\n",
      "9. a: 1094\n",
      "10. slices: 1093\n",
      "11. top: 1037\n",
      "12. place: 783\n",
      "13. pan: 624\n",
      "14. lettuce: 607\n",
      "15. spread: 586\n",
      "16. with: 571\n",
      "17. some: 561\n",
      "18. in: 514\n",
      "19. tomato: 509\n",
      "20. add: 469\n",
      "21. slice: 426\n",
      "22. mayonnaise: 388\n",
      "23. put: 387\n",
      "24. into: 366\n",
      "25. to: 308\n",
      "26. toast: 305\n",
      "27. fry: 291\n",
      "28. tomatoes: 288\n",
      "29. other: 285\n",
      "30. pieces: 270\n",
      "31. cut: 251\n",
      "32. cheese: 238\n",
      "33. it: 236\n",
      "34. sandwich: 217\n",
      "35. take: 216\n",
      "36. lemon: 215\n",
      "37. oil: 209\n",
      "38. cook: 203\n",
      "39. side: 202\n",
      "40. pepper: 201\n",
      "41. salt: 195\n",
      "42. half: 189\n",
      "43. one: 187\n",
      "44. cover: 178\n",
      "45. mustard: 178\n",
      "46. onto: 173\n",
      "47. them: 171\n",
      "48. chicken: 170\n",
      "49. juice: 164\n",
      "50. avocado: 163\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(copy.deepcopy(clean_caps))\n",
    "# print out the most frequent 50 words\n",
    "top_50 = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "for i, (word, count) in enumerate(top_50):\n",
    "    print(f'{i + 1}. {word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae9d6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_caps = []\n",
    "word_count = tokenizer.word_counts\n",
    "for cap in clean_caps:\n",
    "    mcap = []\n",
    "    for word in cap:\n",
    "        if word_count[word] <= 30:\n",
    "            mcap.append('<unk>')\n",
    "        else:\n",
    "            mcap.append(word)\n",
    "    masked_caps.append(mcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "691b0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(copy.deepcopy(masked_caps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5fbc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 0\n",
    "for seq in masked_caps:\n",
    "    max_seq_len = max(len(seq), max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfa9c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_captions(captions, pad_len):\n",
    "    pad_cap = copy.deepcopy(captions)\n",
    "    for caption in pad_cap:\n",
    "        caption += (pad_len - len(caption)) * ['<pad>']\n",
    "    return pad_cap\n",
    "\n",
    "\n",
    "padded_caps = pad_captions(masked_caps, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bca3eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(copy.deepcopy(padded_caps))\n",
    "word2idx, idx2word = {}, {}\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    word2idx[w] = i\n",
    "    idx2word[i] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7298252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_mat = np.array(tokenizer.texts_to_sequences(padded_caps))\n",
    "frames_mat = np.array(all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "873cbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3650, 29)\n",
      "(3650, 512)\n"
     ]
    }
   ],
   "source": [
    "print(caps_mat.shape)\n",
    "print(frames_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ceaa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import PositionalEncoding\n",
    "from model import ImageCaptionModel, accuracy_function, loss_function\n",
    "from decoder import TransformerDecoder, RNNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84058bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 15:56:04.289640: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-01 15:56:04.295505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-01 15:56:04.880786: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training 36/36]\t loss=4.469\t acc: 0.104\t perp: 87.2369\n",
      "[Training 36/36]\t loss=3.125\t acc: 0.306\t perp: 22.760\n",
      "[Training 36/36]\t loss=2.030\t acc: 0.527\t perp: 7.6153\n",
      "[Training 36/36]\t loss=1.253\t acc: 0.711\t perp: 3.500\n",
      "[Training 36/36]\t loss=0.802\t acc: 0.823\t perp: 2.229\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 100\n",
    "hidden_size = 64\n",
    "window_size = 29\n",
    "\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size  = len(word2idx), \n",
    "    hidden_size = hidden_size, \n",
    "    window_size = window_size\n",
    ")\n",
    "\n",
    "model = ImageCaptionModel(decoder)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(\n",
    "    optimizer   = optimizer,\n",
    "    loss        = loss_function,\n",
    "    metrics     = [accuracy_function]\n",
    ")\n",
    "\n",
    "stats = []\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        stats += [model.train(caps_mat, frames_mat, word2idx['<pad>'], batch_size=batch_size)]\n",
    "        if False:\n",
    "            model.test(valid[0], valid[1], pad_idx, batch_size=args.batch_size)\n",
    "except KeyboardInterrupt as e:\n",
    "    if epoch > 0:\n",
    "        print(\"Key-value interruption. Trying to early-terminate. Interrupt again to not do that!\")\n",
    "    else: \n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
