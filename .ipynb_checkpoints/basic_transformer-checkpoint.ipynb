{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e274fdcc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from transformer import PositionalEncoding\n",
    "from model import ImageCaptionModel, accuracy_function, loss_function\n",
    "from decoder import TransformerDecoder, RNNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab5cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformer, model, decoder\n",
    "%aimport transformer, model, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223ac6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './YouCookII/'\n",
    "anno_path = root_path + \"annotations/youcookii_annotations_trainval.json\"\n",
    "feat_path = root_path + 'features/feat_csv/'\n",
    "\n",
    "feat_path_keywords = {'train': 'train_frame_feat_csv', 'val': 'val_frame_feat_csv', 'test': 'test_frame_feat_csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf7b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(anno_path, 'rb')\n",
    "anno_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_types = pd.read_csv(root_path + 'label_foodtype.csv', header=None)\n",
    "idx, types = food_types[0], food_types[1]\n",
    "idx2type = {i: t for i, t in zip(idx, types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0029fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = feat_path + feat_path_keywords['train']\n",
    "val_dir = feat_path + feat_path_keywords['val']\n",
    "test_dir = feat_path + feat_path_keywords['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8a274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': 311.77,\n",
       " 'subset': 'training',\n",
       " 'recipe_type': '101',\n",
       " 'annotations': [{'segment': [41, 54],\n",
       "   'id': 0,\n",
       "   'sentence': 'place the bacon slices on a baking pan and cook them in an oven'},\n",
       "  {'segment': [84, 122],\n",
       "   'id': 1,\n",
       "   'sentence': 'cut the tomatoes into thin slices'},\n",
       "  {'segment': [130, 135],\n",
       "   'id': 2,\n",
       "   'sentence': 'toast the bread slices in the toaster'},\n",
       "  {'segment': [147, 190],\n",
       "   'id': 3,\n",
       "   'sentence': 'spread mayonnaise on the bread and place bacon slices lettuce and tomato slices on top'},\n",
       "  {'segment': [192, 195], 'id': 4, 'sentence': 'top the sandwich with bread'}],\n",
       " 'video_url': 'https://www.youtube.com/watch?v=0O4bxhpFX9o'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_dict['database']['0O4bxhpFX9o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dirs):\n",
    "    all_frames = []\n",
    "    all_caps = []\n",
    "    c_length = []\n",
    "    for dir_name in dirs:\n",
    "        all_vid_names = os.listdir(dir_name)\n",
    "        for vid in all_vid_names:\n",
    "            if vid == '.DS_Store':\n",
    "                continue\n",
    "            vid_dir = dir_name + '/' + vid\n",
    "            vid_feat1 = pd.read_csv(dir_name + '/' + vid + '/0001/resnet_34_feat_mscoco.csv',\n",
    "                                    header=None)\n",
    "            feats = [vid_feat1]\n",
    "            for vid_feat in feats:\n",
    "                vid_segs = anno_dict['database'][vid]['annotations']\n",
    "                vid_len = anno_dict['database'][vid]['duration']\n",
    "                samp_rate = vid_len / 500  # 一般来说这采样率 = length / 一秒多少帧\n",
    "                num_segs = len(vid_segs)\n",
    "                for segments in vid_segs:\n",
    "                    start, end = segments['segment']\n",
    "                    cap = segments['sentence']\n",
    "                    start_fr = int(np.ceil(start / samp_rate))\n",
    "                    end_fr = int(np.floor(end / samp_rate))\n",
    "                    c_length.append(end_fr-start_fr+1)\n",
    "                    frame_idx = []\n",
    "                    # random sample返回的时间顺序会是错的 && 不等距 -> 0 + i * frame gap\n",
    "                    # if end_fr - start_fr < 3:\n",
    "                    #     frame_idx = list(range(start_fr, end_fr + 1))\n",
    "                    #     while len(frame_idx) < 3:\n",
    "                    #         frame_idx += random.sample(list(range(start_fr, end_fr + 1)), 1)\n",
    "                    # else:\n",
    "                    #     frame_idx = random.sample(list(range(start_fr, end_fr + 1)), 3)\n",
    "                    frame_gap = (end_fr - start_fr + 1) // min(3, end_fr - start_fr + 1)\n",
    "                    i = 0\n",
    "                    while i * frame_gap + start_fr <= end_fr:\n",
    "                        frame_idx.append(i * frame_gap + start_fr)\n",
    "                        i += 1\n",
    "                    # 一个segment的frame对应一个caption，而不是把每一帧都当做一个caption\n",
    "                    sub_frames = []\n",
    "                    for frame_num in frame_idx:\n",
    "                        frame = vid_feat.iloc[frame_num]\n",
    "                        sub_frames.append(frame.to_numpy())\n",
    "                    # print(len(sub_frames))\n",
    "                    max_len = 15\n",
    "                    sub_frames = sub_frames[0:max_len]\n",
    "                    padd_num = max_len-len(sub_frames)\n",
    "                    if padd_num > 0:\n",
    "                        for k in range(padd_num):\n",
    "                            padd_instant = np.zeros_like(sub_frames[0])\n",
    "                            sub_frames.append(padd_instant)\n",
    "                    sub_frames = np.array(sub_frames)\n",
    "                    all_frames.append(sub_frames)\n",
    "                    all_caps.append(cap)\n",
    "    print(\"avg_length {}\".format(np.mean(c_length)))\n",
    "    return np.array(all_frames), np.array(all_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393f3ce",
   "metadata": {},
   "source": [
    "# Preprocessing Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293b4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06eddc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions, window_size):\n",
    "    caps_ret = []\n",
    "    for i, caption in enumerate(captions):\n",
    "        # Taken from:\n",
    "        # https://towardsdatascience.com/image-captions-with-attention-in-tensorflow-step-by-step-927dad3569fa\n",
    "\n",
    "        # Convert the caption to lowercase, and then remove all special characters from it\n",
    "        # caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
    "        # TODO: this step can be handled with keras tokenizer?\n",
    "\n",
    "        # Split the caption into separate words, and collect all words which are more than \n",
    "        # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
    "        clean_words = []\n",
    "        for word in caption.split():\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "            elif word.isnumeric():\n",
    "                clean_words.append('<num>')\n",
    "\n",
    "        # Join those words into a string\n",
    "        caption_new = ['<start>'] + clean_words[:window_size - 1] + ['<end>']\n",
    "\n",
    "        # Replace the old caption in the captions list with this new cleaned caption\n",
    "        caps_ret.append(caption_new)\n",
    "    return caps_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392458c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(captions, vocab_size):\n",
    "    word_count = collections.Counter()\n",
    "    for caption in captions:\n",
    "        word_count.update(caption)\n",
    "    \n",
    "    vocab = [word for word, count in word_count.most_common(vocab_size)]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da0ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_captions(captions, vocab):\n",
    "    temp = copy.deepcopy(captions)\n",
    "    for caption in temp:\n",
    "        for index, word in enumerate(caption):\n",
    "            if word not in vocab:\n",
    "                caption[index] = '<unk>'\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa9c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_captions(captions, window_size):\n",
    "    pad_cap = copy.deepcopy(captions)\n",
    "    for caption in pad_cap:\n",
    "        caption += (window_size + 1 - len(caption)) * ['<pad>']\n",
    "    return pad_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e0e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_all(captions_train, captions_test, window_size):\n",
    "    clean_caps_train = preprocess_captions(captions_train, window_size)\n",
    "    clean_caps_test = preprocess_captions(captions_test, window_size)\n",
    "    \n",
    "    vocab = build_vocab(clean_caps_train, 1800)\n",
    "    \n",
    "    masked_caps_train = unk_captions(clean_caps_train, vocab)\n",
    "    masked_caps_test = unk_captions(clean_caps_test, vocab)\n",
    "    \n",
    "    padded_caps_train = pad_captions(masked_caps_train, window_size)\n",
    "    padded_caps_test = pad_captions(masked_caps_test, window_size)\n",
    "    \n",
    "    word2idx = {}\n",
    "    vocab_size = 0\n",
    "    for caption in padded_caps_train:\n",
    "        for index, word in enumerate(caption):\n",
    "            if word in word2idx:\n",
    "                caption[index] = word2idx[word]\n",
    "            else:\n",
    "                word2idx[word] = vocab_size\n",
    "                caption[index] = vocab_size\n",
    "                vocab_size += 1\n",
    "    for caption in padded_caps_test:\n",
    "        for index, word in enumerate(caption):\n",
    "            caption[index] = word2idx[word]\n",
    "    \n",
    "    return np.array(padded_caps_train), np.array(padded_caps_test), word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "445bca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subdir = os.listdir(train_dir)\n",
    "# train_subdir.remove('.DS_Store')\n",
    "train_paths = [os.path.join(train_dir, subdir) for subdir in train_subdir]\n",
    "val_paths = [os.path.join(val_dir, subdir) for subdir in train_subdir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ceaa5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_length 31.420334719938086\n",
      "avg_length 32.19730813287514\n"
     ]
    }
   ],
   "source": [
    "train_frames, train_caps = get_data(train_paths)\n",
    "val_frames, val_caps = get_data(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9113c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caps_token, val_caps_token, word2idx = preproc_all(train_caps, val_caps, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0477a10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10337,) (3492,)\n",
      "(10337, 21) (3492, 21)\n"
     ]
    }
   ],
   "source": [
    "print(train_caps.shape, val_caps.shape)\n",
    "print(train_caps_token.shape, val_caps_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84058bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最好用TFVisionEncoderDecoderModel\n",
    "def train_model(train, word2idx, epochs, batch_size, hidden_size, window_size, valid=None):\n",
    "    print('train[0] shape', train[0].shape)\n",
    "    print('train[1] shape', train[1].shape)\n",
    "\n",
    "    decoder = TransformerDecoder(\n",
    "        vocab_size  = len(word2idx), \n",
    "        hidden_size = hidden_size, \n",
    "        window_size = window_size\n",
    "    )\n",
    "\n",
    "    model = ImageCaptionModel(decoder)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer   = optimizer,\n",
    "        loss        = loss_function,\n",
    "        metrics     = [accuracy_function]\n",
    "    )\n",
    "\n",
    "    stats = []\n",
    "    for epoch in range(epochs):\n",
    "        stats += [model.train(train[0], train[1], word2idx['<pad>'], batch_size=batch_size)]\n",
    "        if valid:\n",
    "            model.test(valid[0], valid[1], word2idx['<pad>'], batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6cc4b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train[0] shape (10337, 21)\n",
      "train[1] shape (10337, 15, 512)\n",
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 13:29:19.252328: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-08 13:29:19.252962: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-12-08 13:29:20.181212: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-08 13:29:20.189026: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-08 13:29:20.637192: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valid 1/54]\t loss=6.006\t acc: 0.177\t perp: 405.8553263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 13:29:36.683857: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valid 54/54]\t loss=5.981\t acc: 0.173\t perp: 396.001\n",
      "[Valid 54/54]\t loss=5.130\t acc: 0.222\t perp: 169.02130\n",
      "[Valid 54/54]\t loss=4.635\t acc: 0.225\t perp: 103.06958\n",
      "[Valid 54/54]\t loss=4.302\t acc: 0.248\t perp: 73.82934\n",
      "[Valid 54/54]\t loss=4.085\t acc: 0.268\t perp: 59.44277\n",
      "[Valid 54/54]\t loss=3.925\t acc: 0.285\t perp: 50.63004\n",
      "[Valid 54/54]\t loss=3.790\t acc: 0.299\t perp: 44.27133\n",
      "[Valid 54/54]\t loss=3.681\t acc: 0.308\t perp: 39.68376\n",
      "[Valid 54/54]\t loss=3.596\t acc: 0.321\t perp: 36.44635\n",
      "[Valid 54/54]\t loss=3.525\t acc: 0.332\t perp: 33.96209\n",
      "[Valid 54/54]\t loss=3.473\t acc: 0.336\t perp: 32.23327\n",
      "[Valid 54/54]\t loss=3.422\t acc: 0.343\t perp: 30.61905\n",
      "[Valid 54/54]\t loss=3.386\t acc: 0.347\t perp: 29.54064\n",
      "[Valid 54/54]\t loss=3.348\t acc: 0.352\t perp: 28.45313\n",
      "[Valid 54/54]\t loss=3.322\t acc: 0.355\t perp: 27.71689\n",
      "[Valid 54/54]\t loss=3.295\t acc: 0.358\t perp: 26.97737\n",
      "[Valid 54/54]\t loss=3.279\t acc: 0.361\t perp: 26.56138\n",
      "[Valid 54/54]\t loss=3.256\t acc: 0.363\t perp: 25.93841\n",
      "[Valid 54/54]\t loss=3.239\t acc: 0.365\t perp: 25.51684\n",
      "[Valid 54/54]\t loss=3.227\t acc: 0.367\t perp: 25.21445\n",
      "[Valid 54/54]\t loss=3.216\t acc: 0.368\t perp: 24.93040\n",
      "[Valid 54/54]\t loss=3.204\t acc: 0.370\t perp: 24.64277\n",
      "[Valid 54/54]\t loss=3.195\t acc: 0.370\t perp: 24.40698\n",
      "[Valid 54/54]\t loss=3.189\t acc: 0.372\t perp: 24.26002\n",
      "[Valid 54/54]\t loss=3.186\t acc: 0.371\t perp: 24.19574\n",
      "[Valid 54/54]\t loss=3.175\t acc: 0.372\t perp: 23.93140\n",
      "[Valid 54/54]\t loss=3.166\t acc: 0.374\t perp: 23.71400\n",
      "[Valid 54/54]\t loss=3.161\t acc: 0.373\t perp: 23.60633\n",
      "[Valid 54/54]\t loss=3.161\t acc: 0.374\t perp: 23.59953\n",
      "[Valid 54/54]\t loss=3.154\t acc: 0.375\t perp: 23.43912\n",
      "[Valid 54/54]\t loss=3.152\t acc: 0.373\t perp: 23.37678\n",
      "[Valid 54/54]\t loss=3.150\t acc: 0.376\t perp: 23.34561\n",
      "[Valid 54/54]\t loss=3.151\t acc: 0.372\t perp: 23.35778\n",
      "[Valid 54/54]\t loss=3.148\t acc: 0.373\t perp: 23.28306\n",
      "[Valid 54/54]\t loss=3.149\t acc: 0.374\t perp: 23.31327\n",
      "[Valid 54/54]\t loss=3.149\t acc: 0.375\t perp: 23.32472\n",
      "[Valid 54/54]\t loss=3.151\t acc: 0.376\t perp: 23.37059\n",
      "[Valid 54/54]\t loss=3.157\t acc: 0.375\t perp: 23.50791\n",
      "[Valid 54/54]\t loss=3.153\t acc: 0.375\t perp: 23.40789\n",
      "[Valid 54/54]\t loss=3.155\t acc: 0.374\t perp: 23.45664\n",
      "[Valid 54/54]\t loss=3.158\t acc: 0.375\t perp: 23.52760\n",
      "[Valid 54/54]\t loss=3.167\t acc: 0.375\t perp: 23.72566\n",
      "[Valid 54/54]\t loss=3.168\t acc: 0.375\t perp: 23.75286\n",
      "[Valid 54/54]\t loss=3.173\t acc: 0.375\t perp: 23.88082\n",
      "[Valid 54/54]\t loss=3.175\t acc: 0.373\t perp: 23.92408\n",
      "[Valid 54/54]\t loss=3.183\t acc: 0.373\t perp: 24.12733\n",
      "[Valid 54/54]\t loss=3.185\t acc: 0.372\t perp: 24.16256\n",
      "[Valid 54/54]\t loss=3.192\t acc: 0.373\t perp: 24.33908\n",
      "[Valid 54/54]\t loss=3.196\t acc: 0.372\t perp: 24.43551\n",
      "[Valid 54/54]\t loss=3.207\t acc: 0.370\t perp: 24.69501\n",
      "[Valid 54/54]\t loss=3.207\t acc: 0.370\t perp: 24.70066\n",
      "[Valid 54/54]\t loss=3.215\t acc: 0.372\t perp: 24.90610\n",
      "[Valid 54/54]\t loss=3.223\t acc: 0.371\t perp: 25.1139\n",
      "[Valid 54/54]\t loss=3.226\t acc: 0.371\t perp: 25.1727\n",
      "[Valid 54/54]\t loss=3.232\t acc: 0.371\t perp: 25.3220\n",
      "[Valid 54/54]\t loss=3.245\t acc: 0.370\t perp: 25.6493\n",
      "[Valid 54/54]\t loss=3.247\t acc: 0.369\t perp: 25.7118\n",
      "[Valid 54/54]\t loss=3.251\t acc: 0.370\t perp: 25.8219\n",
      "[Valid 54/54]\t loss=3.263\t acc: 0.370\t perp: 26.1323\n",
      "[Valid 54/54]\t loss=3.274\t acc: 0.367\t perp: 26.4059\n"
     ]
    }
   ],
   "source": [
    "model = train_model((train_caps_token, train_frames),\n",
    "                    word2idx,\n",
    "                    epochs=60,\n",
    "                    batch_size=64,\n",
    "                    hidden_size=128, #256 #block num <=2\n",
    "                    window_size=window_size,\n",
    "                    valid=(val_caps_token, val_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3af5bc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10337, 15, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d69eecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens [1 2 3 4 5 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "tea [1, 4, 3, 4, 145, 6, 6, 9, 9, 9, 9, 435, 435, 435, 546, 546, 546, 546, 9, 6]\n",
      "tokens [ 1  8  9 10  3  4  2  6  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [14, 4, 25, 10, 3, 4, 65, 6, 6, 3, 9, 435, 435, 9, 546, 546, 546, 546, 3, 6]\n",
      "tokens [ 1 11  3  4  2  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [14, 4, 3, 4, 108, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 546, 9, 9, 6]\n",
      "tokens [12 13  4  5  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [14, 13, 4, 253, 9, 31, 9, 9, 9, 9, 10, 10, 10, 10, 10, 546, 546, 546, 9, 6]\n",
      "tokens [14  4 12 15 16 17 18  6  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [14, 4, 1097, 31, 4, 18, 18, 6, 193, 9, 9, 9, 9, 10, 9, 546, 546, 9, 9, 6]\n",
      "tokens [19  4  2 20  4  5  6  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [1, 4, 181, 25, 4, 253, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "tokens [21  4  5 22  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [14, 4, 1096, 22, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6]\n",
      "tokens [23  4 11  9 24 25  4  5 26 16 27  6  7  7  7  7  7  7  7  7]\n",
      "tea [14, 4, 152, 9, 39, 25, 4, 233, 6, 4, 178, 6, 6, 6, 9, 9, 9, 9, 6, 6]\n",
      "tokens [12 13  9 28 29 15  4 30  6  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [23, 4, 9, 1, 29, 6, 4, 253, 6, 6, 9, 9, 181, 148, 546, 546, 546, 9, 6, 6]\n",
      "tokens [14  4 12 31 32  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "tea [23, 4, 12, 31, 131, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "def gen_caption_temperature(model, image_embedding, tokens, wordToIds, padID, temp, window_length):\n",
    "    \"\"\"\n",
    "    Function used to generate a caption using an ImageCaptionModel given\n",
    "    an image embedding. \n",
    "    \"\"\"\n",
    "    print('tokens', tokens[1:])  # answer\n",
    "    idsToWords = {id: word for word, id in wordToIds.items()}\n",
    "    unk_token = wordToIds['<unk>']\n",
    "    caption_so_far = [wordToIds['<start>']]\n",
    "    teacher_based_out = []\n",
    "    inp = np.array([tokens[0:-1]])\n",
    "    logits = model(np.expand_dims(image_embedding, 0), inp)\n",
    "    probs = tf.nn.softmax(logits).numpy()\n",
    "    teacher_based_out = np.argmax(probs, axis=2)\n",
    "    print(\"tea\", list(teacher_based_out[0]))  # prediction\n",
    "    # 前几个word的预测效果很差导致最后采样生成的句子偏差很大, \n",
    "    # 1. 模型：teaching forcing loss本身的缺陷 \n",
    "    # 2. 预处理：图片信息的预处理有问题，信息没有被有效的利用\n",
    "    # 3. dataset本身：可能作者自己都没有跑过这个task，只提出了dataset，他们自己也不知道baseline；或者这个dataset本身就不可解\n",
    "    while len(caption_so_far) < window_length and caption_so_far[-1] != wordToIds['<end>']:\n",
    "        caption_input = np.array([caption_so_far + ((window_length - len(caption_so_far)) * [padID])])\n",
    "        logits = model(np.expand_dims(image_embedding, 0), caption_input)\n",
    "        logits = logits[0][len(caption_so_far) - 1]\n",
    "        probs = tf.nn.softmax(logits / temp).numpy()\n",
    "        next_token = unk_token\n",
    "        attempts = 0\n",
    "        while next_token == unk_token and attempts < 5:\n",
    "            next_token = np.random.choice(len(probs), p=probs)\n",
    "            next_token =np.argmax(probs)\n",
    "            attempts += 1\n",
    "        caption_so_far.append(next_token)\n",
    "    return ' '.join([idsToWords[x] for x in caption_so_far][1:])\n",
    "\n",
    "\n",
    "temperature = .05\n",
    "for i in range(10):\n",
    "    t = gen_caption_temperature(model, train_frames[i], train_caps_token[i], word2idx, word2idx['<pad>'], temperature, window_size)\n",
    "    #print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "023016a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add rice to the seaweed', 'add avocado and cucumber to the rice',\n",
       "       'add crab to the rice', 'roll up the seaweed',\n",
       "       'cut the roll with a sharp knife',\n",
       "       'spread the rice onto the seaweed', 'flip the seaweed over',\n",
       "       'place the crab and celery on the seaweed in a line',\n",
       "       'roll up and press down with the mat', 'cut the roll into pieces'],\n",
       "      dtype='<U226')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_caps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2182b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf948e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d48df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada5850-8f5b-41f2-b172-06b560562e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
